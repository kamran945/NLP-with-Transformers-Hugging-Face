{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1217821,"sourceType":"datasetVersion","datasetId":695933}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Sentiment Analysis of Stock Market News Data\n* The goal is to classify text into categories such as positive or negative.\n\n## Models for Sentiment Analysis\n1. **Baseline Model: Logistic Regression**: A baseline model that uses traditional machine learning techniques with text features. \n2. **Intermediate Model: Recurrent Neural Networks (RNNs) with LSTM**:  Captures sequential dependencies and context in text.\n3. **Advanced Model: Transformers (BERT etc.)**: Provides state-of-the-art performance by leveraging deep contextual understanding.","metadata":{}},{"cell_type":"markdown","source":"# Get Utils file from Github \nhttps://github.com/kamran945/NLP-Text-Classification/raw/main/nlp_sentiment_utils.py","metadata":{}},{"cell_type":"code","source":"!pip install contractions # required in utils file","metadata":{"execution":{"iopub.status.busy":"2024-08-23T15:21:23.203565Z","iopub.execute_input":"2024-08-23T15:21:23.203965Z","iopub.status.idle":"2024-08-23T15:21:41.112419Z","shell.execute_reply.started":"2024-08-23T15:21:23.203923Z","shell.execute_reply":"2024-08-23T15:21:41.111069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\nfile_name = \"nlp_sentiment_utils.py\"\ngithub_url = \"https://github.com/kamran945/NLP-Text-Classification/raw/main/nlp_sentiment_utils.py\"\n\nif not os.path.exists(file_name):\n    print(f\"{file_name} not found. Downloading from GitHub...\")\n    !wget {github_url} -O {file_name}\nelse:\n    print(f\"{file_name} already exists. No need to download.\")\n\nimport nlp_sentiment_utils","metadata":{"execution":{"iopub.status.busy":"2024-08-23T15:21:41.114017Z","iopub.execute_input":"2024-08-23T15:21:41.114418Z","iopub.status.idle":"2024-08-23T15:21:44.772861Z","shell.execute_reply.started":"2024-08-23T15:21:41.114377Z","shell.execute_reply":"2024-08-23T15:21:44.771559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load and Explore the data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv(\"/kaggle/input/stockmarket-sentiment-dataset/stock_data.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-23T15:21:44.776051Z","iopub.execute_input":"2024-08-23T15:21:44.776633Z","iopub.status.idle":"2024-08-23T15:21:44.833934Z","shell.execute_reply.started":"2024-08-23T15:21:44.776588Z","shell.execute_reply":"2024-08-23T15:21:44.832932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check distribution of classes\nimport matplotlib.pyplot as plt\n\ndf[\"Sentiment\"].value_counts(normalize=True).plot(kind='bar')\nplt.xlabel('Sentiment')\nplt.ylabel('Frequency')\nplt.title('Sentiment Frequency Distribution');","metadata":{"execution":{"iopub.status.busy":"2024-08-23T15:21:44.835126Z","iopub.execute_input":"2024-08-23T15:21:44.835489Z","iopub.status.idle":"2024-08-23T15:21:45.183952Z","shell.execute_reply.started":"2024-08-23T15:21:44.835451Z","shell.execute_reply":"2024-08-23T15:21:45.182922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"Sentiment\"].replace(-1, 0, inplace=True) # replace -1 with 0\ndf.rename(columns={'Text': 'text'}, inplace=True) # rename 'Text' column to 'text'\ndf.tail()","metadata":{"execution":{"iopub.status.busy":"2024-08-23T15:21:45.185660Z","iopub.execute_input":"2024-08-23T15:21:45.186140Z","iopub.status.idle":"2024-08-23T15:21:45.200443Z","shell.execute_reply.started":"2024-08-23T15:21:45.186086Z","shell.execute_reply":"2024-08-23T15:21:45.199322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Clean Text","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\nimport re\n\nstop_words = stopwords.words('english')\nimportant_stop_words = ['not', 'no', 'nor', \"isn't\", \"aren't\", \"wasn't\", \"weren't\", \"haven't\", \"hasn't\", \"hadn't\", \"doesn't\", \"don't\", \"didn't\", \"won't\", \"wouldn't\", \"shan't\", \"shouldn't\", \"can't\", \"couldn't\", \"mustn't\", \"mightn't\", \"needn't\"]\n\nstop_words = [word for word in stop_words if not re.match(r'^(not|no|nor|\\w*\\'?n?t|[\\w]*n$)$', word)]\nprint(stop_words)","metadata":{"execution":{"iopub.status.busy":"2024-08-23T15:21:45.201803Z","iopub.execute_input":"2024-08-23T15:21:45.202212Z","iopub.status.idle":"2024-08-23T15:21:45.216471Z","shell.execute_reply.started":"2024-08-23T15:21:45.202160Z","shell.execute_reply":"2024-08-23T15:21:45.215148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = nlp_sentiment_utils.clean_text(df, column=\"text\")\ndf","metadata":{"execution":{"iopub.status.busy":"2024-08-23T15:21:45.217825Z","iopub.execute_input":"2024-08-23T15:21:45.218414Z","iopub.status.idle":"2024-08-23T15:21:45.968306Z","shell.execute_reply.started":"2024-08-23T15:21:45.218370Z","shell.execute_reply":"2024-08-23T15:21:45.967157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Vocabulary","metadata":{}},{"cell_type":"code","source":"vocabulary = nlp_sentiment_utils.get_vocabulary(df, column='text')","metadata":{"execution":{"iopub.status.busy":"2024-08-23T15:21:45.972611Z","iopub.execute_input":"2024-08-23T15:21:45.972965Z","iopub.status.idle":"2024-08-23T15:21:45.995441Z","shell.execute_reply.started":"2024-08-23T15:21:45.972929Z","shell.execute_reply":"2024-08-23T15:21:45.994155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reduced_vocab = nlp_sentiment_utils.reduce_vocabulary(vocabulary, quantile=0.95)","metadata":{"execution":{"iopub.status.busy":"2024-08-23T15:21:45.996960Z","iopub.execute_input":"2024-08-23T15:21:45.997453Z","iopub.status.idle":"2024-08-23T15:21:46.012340Z","shell.execute_reply.started":"2024-08-23T15:21:45.997401Z","shell.execute_reply":"2024-08-23T15:21:46.011157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### View Vocabualry","metadata":{}},{"cell_type":"code","source":"nlp_sentiment_utils.plot_wordcloud(vocabulary, title='Both Classes: Word Cloud')","metadata":{"execution":{"iopub.status.busy":"2024-08-23T15:21:46.013996Z","iopub.execute_input":"2024-08-23T15:21:46.015090Z","iopub.status.idle":"2024-08-23T15:21:47.245691Z","shell.execute_reply.started":"2024-08-23T15:21:46.015047Z","shell.execute_reply":"2024-08-23T15:21:47.244583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get vocabulary for different classes\n\nvocab_class_1 = nlp_sentiment_utils.get_vocabulary(df[df[\"Sentiment\"] == 1])\nvocab_class_0 = nlp_sentiment_utils.get_vocabulary(df[df[\"Sentiment\"] == 0])\nnlp_sentiment_utils.plot_wordcloud(vocab_class_1, title='Class 1: Word Cloud')\nnlp_sentiment_utils.plot_wordcloud(vocab_class_0, title='Class 0: Word Cloud')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-23T15:21:47.247099Z","iopub.execute_input":"2024-08-23T15:21:47.247465Z","iopub.status.idle":"2024-08-23T15:21:49.668560Z","shell.execute_reply.started":"2024-08-23T15:21:47.247427Z","shell.execute_reply":"2024-08-23T15:21:49.667415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Train, Validation and Test Splits","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = df['text']  # Feature\ny = df['Sentiment']  # Target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n                                                    stratify=y, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, \n                                                stratify=y_test, random_state=42)\n\nlen(X_train), len(y_train), len(X_val), len(y_val), len(X_test), len(y_test)","metadata":{"execution":{"iopub.status.busy":"2024-08-23T15:21:49.670194Z","iopub.execute_input":"2024-08-23T15:21:49.670645Z","iopub.status.idle":"2024-08-23T15:21:49.691812Z","shell.execute_reply.started":"2024-08-23T15:21:49.670599Z","shell.execute_reply":"2024-08-23T15:21:49.690684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nnp.sum(y_train == 1) / len(y_train), np.sum(y_val == 1) / len(y_val), np.sum(y_test == 1) / len(y_test)","metadata":{"execution":{"iopub.status.busy":"2024-08-23T15:21:49.693602Z","iopub.execute_input":"2024-08-23T15:21:49.694381Z","iopub.status.idle":"2024-08-23T15:21:49.703335Z","shell.execute_reply.started":"2024-08-23T15:21:49.694324Z","shell.execute_reply":"2024-08-23T15:21:49.702169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Baseline: Logistic Regression Model","metadata":{}},{"cell_type":"markdown","source":"## Count Vectorizer\n* CountVectorizer implements the Bag of Words (BoW) model\n* **BoW Model**: Represents text data as a collection of words and their frequencies, ignoring grammar and word order. **Counts the occurrences of each word** in the documents and converts these counts into a numerical feature matrix.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\n\nvectorizer_count = CountVectorizer() \n\nX_train_count = vectorizer_count.fit_transform(X_train)\n\nX_val_count = vectorizer_count.transform(X_val)\nX_test_count = vectorizer_count.transform(X_test)\n\nmodel = LogisticRegression(class_weight='balanced')\nmodel.fit(X_train_count, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-08-23T15:21:49.705083Z","iopub.execute_input":"2024-08-23T15:21:49.705483Z","iopub.status.idle":"2024-08-23T15:21:49.954880Z","shell.execute_reply.started":"2024-08-23T15:21:49.705444Z","shell.execute_reply":"2024-08-23T15:21:49.953675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, classification_report\nfrom collections import defaultdict\n\neval_metrics = defaultdict(dict)\n\ny_pred = model.predict(X_val_count)\neval_metrics.update({'logistic_reg_count': nlp_sentiment_utils.get_eval_metrics(y_val, y_pred)})\nprint(classification_report(y_val, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-08-23T15:21:49.956184Z","iopub.execute_input":"2024-08-23T15:21:49.956578Z","iopub.status.idle":"2024-08-23T15:21:49.983236Z","shell.execute_reply.started":"2024-08-23T15:21:49.956525Z","shell.execute_reply":"2024-08-23T15:21:49.982214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TF-TDF Vectorizer","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer_tfidf = TfidfVectorizer()\n\nX_train_tfidf = vectorizer_tfidf.fit_transform(X_train)\n\nX_val_tfidf = vectorizer_tfidf.transform(X_val)\nX_test_tfidf = vectorizer_tfidf.transform(X_test)\n\nmodel = LogisticRegression(class_weight='balanced')\nmodel.fit(X_train_tfidf, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-08-23T15:21:49.984818Z","iopub.execute_input":"2024-08-23T15:21:49.985719Z","iopub.status.idle":"2024-08-23T15:21:50.181161Z","shell.execute_reply.started":"2024-08-23T15:21:49.985665Z","shell.execute_reply":"2024-08-23T15:21:50.179664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, classification_report\nfrom collections import defaultdict\n\ny_pred = model.predict(X_val_tfidf)\neval_metrics.update({'logistic_reg_tfidf': nlp_sentiment_utils.get_eval_metrics(y_val, y_pred)})\nprint(classification_report(y_val, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-08-23T15:21:50.183358Z","iopub.execute_input":"2024-08-23T15:21:50.183881Z","iopub.status.idle":"2024-08-23T15:21:50.211449Z","shell.execute_reply.started":"2024-08-23T15:21:50.183816Z","shell.execute_reply":"2024-08-23T15:21:50.210275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval_metrics","metadata":{"execution":{"iopub.status.busy":"2024-08-23T15:21:50.213299Z","iopub.execute_input":"2024-08-23T15:21:50.213741Z","iopub.status.idle":"2024-08-23T15:21:50.220880Z","shell.execute_reply.started":"2024-08-23T15:21:50.213689Z","shell.execute_reply":"2024-08-23T15:21:50.219763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RNN Model (LSTM Based) using PyTorch","metadata":{}},{"cell_type":"markdown","source":"## Prepare the data for Deep Learning Model","metadata":{}},{"cell_type":"markdown","source":"## LSTM Model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass LSTMClassifierWithEmbedding(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, \n                 hidden_size, num_classes, num_layers=1, \n                 dropout=0.5):\n        \n        super(LSTMClassifierWithEmbedding, self).__init__()\n        \n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, \n                            hidden_size, \n                            num_layers, # number of stacked layers in lstm\n                            batch_first=True, # first dimension represents the batch size\n                            dropout=dropout,\n                            bidirectional=False)\n        self.fc = nn.Linear(hidden_size, num_classes)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x):\n        # Apply embedding layer\n        x = self.embedding(x)\n        \n        # Forward pass through LSTM\n        lstm_out, _ = self.lstm(x) \n        \n        # Take the output from the last time step\n        # Last step in the sequence represents the entire sequence\n        # This is generally done for classification tasks\n        lstm_out = lstm_out[:, -1, :] \n        \n        # Apply dropout\n        lstm_out = self.dropout(lstm_out)\n        \n        # Fully connected layer\n        out = self.fc(lstm_out)\n        \n        return out\n","metadata":{"execution":{"iopub.status.busy":"2024-08-23T15:21:50.222190Z","iopub.execute_input":"2024-08-23T15:21:50.222564Z","iopub.status.idle":"2024-08-23T15:21:53.497014Z","shell.execute_reply.started":"2024-08-23T15:21:50.222528Z","shell.execute_reply":"2024-08-23T15:21:53.495664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# work in progress\n\n\n\n# import torch\n# import torch.nn as nn\n# import torch.optim as optim\n\n# vocab_size = reduced_vocabulary\n# embedding_dim = 256\n# hidden_size = 128\n# num_classes = 2\n# num_layers = 3\n\n# model_lstm = LSTMClassifierWithEmbedding(vocab_size, embedding_dim, \n#                                          hidden_size, num_classes,\n#                                          num_layers=num_layers)\n# loss_fcn = nn.CrossEntropyLoss()\n# optimizer = optim.Adam(model_lstm.parameters(), lr=0.001)\n\n# # Training loop\n# num_epochs = 10\n# for epoch in range(num_epochs):\n#     for inputs, labels in dataloader:\n#         # Forward pass\n#         outputs = model_lstm(inputs)\n#         loss = loss_fcn(outputs, labels)\n        \n#         # Backward pass and optimization\n#         optimizer.zero_grad()\n#         loss.backward()\n#         optimizer.step()","metadata":{},"execution_count":null,"outputs":[]}]}